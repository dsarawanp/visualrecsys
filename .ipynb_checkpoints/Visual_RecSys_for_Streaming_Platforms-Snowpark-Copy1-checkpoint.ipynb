{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8smXSSFk4cmP"
   },
   "source": [
    "# Visual RecSys for Streaming Platforms\n",
    "\n",
    "Visual similarity recommendation refers to the process of suggesting items or content based on their visual similarity to a reference item. This type of recommendation system is commonly used in various domains, such as e-commerce, image search engines, and content recommendation platforms.\n",
    "\n",
    "Methodology:-\n",
    "1. Data Collection\n",
    "2. Feature Extraction\n",
    "3. Similarity Calculation\n",
    "4. Ranking and Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AL1QUwA60B4"
   },
   "source": [
    "## 1.Problem\n",
    "To recommend movie posters from the dataset of movies posters given an image of the movie poster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdwAVex85yNc"
   },
   "source": [
    "## 2.Data Collection\n",
    "We are using the dataset taken from the Kaggle\n",
    "https://www.kaggle.com/datasets/akshaypawar7/millions-of-movies\n",
    "\n",
    "We have used the refined movie dataset `movies.csv`\n",
    " shared along with this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3OcrqWfaER9"
   },
   "source": [
    "### Loading the data from the movie excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b79uzy6z62ID"
   },
   "outputs": [],
   "source": [
    "# loading the data from the movie excel \n",
    "import pandas as pd\n",
    "from smart_open import open\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=visrecstorage;AccountKey=q3Wvmg9bF4oPqZYdXV6PJ2+XPDfD3z4FckngdyHGMyCGE5zHMgqKPVNVk3AxGdjERc28EHGBVEE2+AStDSPpVw==;EndpointSuffix=core.windows.net'\n",
    "transport_params = {\n",
    "    'client': BlobServiceClient.from_connection_string(connect_str),\n",
    "}\n",
    "\n",
    "csvfile = open(\"azure://visrec/movies.csv\", 'rb', transport_params=transport_params)\n",
    "moviedata = pd.read_csv(csvfile, delimiter = ',')\n",
    "moviedata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Gets the version\n",
    "ctx = snowflake.connector.connect(user='SARAWANPERNETI193',password='@Temp2023',account='hb81971.ca-central-1.aws')\n",
    "cs = ctx.cursor()\n",
    "try:\n",
    "    cs.execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "    cs.execute(\"DROP DATABASE IF EXISTS MOVIES_DB\")\n",
    "    cs.execute(\"CREATE DATABASE MOVIES_DB\")\n",
    "    cs.execute(\"DROP SCHEMA IF EXISTS MOVIES_TABLES\")\n",
    "    cs.execute(\"CREATE SCHEMA MOVIES_TABLES\")\n",
    "finally:\n",
    "    cs.close()\n",
    "ctx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": 'hb81971.ca-central-1.aws',\n",
    "    \"user\": 'SARAWANPERNETI193',\n",
    "    \"password\": '@Temp2023',\n",
    "    \"role\": \"ACCOUNTADMIN\",\n",
    "    \"warehouse\": \"COMPUTE_WH\",\n",
    "    \"database\": \"MOVIES_DB\",\n",
    "    \"schema\": \"MOVIES_TABLES\"\n",
    "    \n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azQ6m-FeZ8zt"
   },
   "source": [
    "### Loading the data to the database into the movie table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5vvSStDMOaN"
   },
   "outputs": [],
   "source": [
    "#Loading the data to the database into the movie table \n",
    "try:\n",
    "    ctx = snowflake.connector.connect(user='SARAWANPERNETI193',password='@Temp2023',account='hb81971.ca-central-1.aws',role=\"ACCOUNTADMIN\",warehouse=\"COMPUTE_WH\",database=\"MOVIES_DB\",schema=\"MOVIES_TABLES\")\n",
    "    cursor = ctx.cursor()\n",
    "    cursor.execute('DROP TABLE IF EXISTS MOVIE_DATA;')\n",
    "    print('Creating table....')\n",
    "    # in the below line please pass the create table statement which you want #to create\n",
    "    cursor.execute(\"CREATE TABLE MOVIE_DATA(movie_id int primary key,movie_title TEXT, genres TEXT, original_language varchar(255),overview TEXT,production TEXT,release_date varchar(255),runtime varchar(255),voter_rating varchar(255),voters_count varchar(255),credits TEXT,keywords TEXT,Poster_path varchar(255))\")\n",
    "    print(\"Table is created....\")\n",
    "    #loop through the data frame\n",
    "    for i,row in moviedata.iterrows():\n",
    "        #here %S means string values \n",
    "        sql = \"INSERT INTO MOVIE_DATA VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        cursor.execute(sql, list(row))\n",
    "        print(\"Record inserted:-\" + row[1])\n",
    "    cursor.execute(\"ALTER TABLE movie_data add poster_image binary;\")\n",
    "    print(\"Table is altered....\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "ctx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0Cu2e-rRBDM"
   },
   "source": [
    "### Downloading and inserting the poster images to the movie table in the movie database created\n",
    "\n",
    "The below code can be tweak by setting the limits to the movie id column such that we can insert the desired number of movie poster images required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoXIxmJARMZi"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "from smart_open import open\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=visrecstorage;AccountKey=q3Wvmg9bF4oPqZYdXV6PJ2+XPDfD3z4FckngdyHGMyCGE5zHMgqKPVNVk3AxGdjERc28EHGBVEE2+AStDSPpVw==;EndpointSuffix=core.windows.net'\n",
    "transport_params = {\n",
    "    'client': BlobServiceClient.from_connection_string(connect_str),\n",
    "}\n",
    "\n",
    "\n",
    "def convert_data(file_name):\n",
    "    with open(file_name, 'rb', transport_params=transport_params) as file:\n",
    "        binary_data = file.read()\n",
    "    return binary_data\n",
    "\n",
    "try:\n",
    "    ctx = snowflake.connector.connect(user='SARAWANPERNETI193',password='@Temp2023',account='hb81971.ca-central-1.aws',role=\"ACCOUNTADMIN\",warehouse=\"COMPUTE_WH\",database=\"MOVIES_DB\",schema=\"MOVIES_TABLES\")\n",
    "    cursor = ctx.cursor()\n",
    "    cursor.execute(\"SELECT * FROM MOVIE_DATA where movie_id > 0 and  movie_id < 100000\")\n",
    "    myresult = cursor.fetchall()\n",
    "    print(len(myresult))\n",
    "    for x in myresult:\n",
    "        filename = \"azure://posters/\"+str(x[0])+\".jpg\"\n",
    "        # Open the url image, set stream to True, this will return the stream content.\n",
    "        print(str(x[0])+\":- \"+x[1] + \":-\"+ x[12])\n",
    "        r = requests.get(x[12], stream = True)\n",
    "        # Check if the image was retrieved successfully\n",
    "        if r.status_code == 200:\n",
    "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "            r.raw.decode_content = True\n",
    "            # Open a local file with wb ( write binary ) permission.\n",
    "            with open(filename,'wb', transport_params=transport_params) as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "            imagedata=convert_data(filename)\n",
    "            sql = \"Update movie_data set poster_image = %s where movie_id = %s\"\n",
    "            cursor.execute(sql, (imagedata, x[0]))\n",
    "            print(str(x[0])+\":- \"+x[1] + \" :- image inserted\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "ctx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPJ7PgDDRdUQ"
   },
   "source": [
    "## 3.Feature Extraction\n",
    "Here we are using the `ResNet50` model and `ImageNet` weights for the feature extraction through transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfml5iwPZZZ6"
   },
   "source": [
    "### `ResNet50` \n",
    "ResNet-50 is a convolutional neural network (CNN) model, that has a deep architecture consisting of 50 layers, including convolutional layers, pooling layers, fully connected layers, and shortcut connections known as skip connections.\n",
    "\n",
    "The input to ResNet-50 is typically a 224x224 RGB image, and the output is a vector of probabilities representing the predicted probabilities of different classes. The model is trained using a large dataset, such as ImageNet, where it learns to classify images into one of the 1,000 predefined classes. ResNet-50 has also been used as a starting point for transfer learning, where the pre-trained model is fine-tuned on a specific task using a smaller dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "561M2r6eZjU5"
   },
   "source": [
    "###`ImageNet`\n",
    "The ImageNet weights refer to the pre-trained weights of a neural network model, specifically trained on the ImageNet dataset. The ImageNet dataset is a large-scale dataset containing millions of labeled images belonging to thousands of different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afAILbxZT1Er"
   },
   "source": [
    "\n",
    "### Genres list\n",
    "\n",
    "There are approximation of 19 genres associated in the movie data base \n",
    "The following are the list of the genres \n",
    "\n",
    "1.Action, \n",
    "2.Adventure,\n",
    "3.Animation,\n",
    "4.Comedy,\n",
    "5.Crime,\n",
    "6.Documentary,\n",
    "7.Drama,\n",
    "8.Family,\n",
    "9.Fantasy,\n",
    "10.History,\n",
    "11.Horror,\n",
    "12.Music,\n",
    "13.Mystery,\n",
    "14.Romance,\n",
    "15.Science Fiction,\n",
    "16.Thriller,\n",
    "17.TV Movie,\n",
    "18.War,\n",
    "19.Western,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLGNoHygZpdW"
   },
   "source": [
    "### Genre wise feature extraction from the poster images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox8qp80sTVwE"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from smart_open import open\n",
    "\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=visrecstorage;AccountKey=q3Wvmg9bF4oPqZYdXV6PJ2+XPDfD3z4FckngdyHGMyCGE5zHMgqKPVNVk3AxGdjERc28EHGBVEE2+AStDSPpVw==;EndpointSuffix=core.windows.net'\n",
    "transport_params = {\n",
    "    'client': BlobServiceClient.from_connection_string(connect_str),\n",
    "}\n",
    "\n",
    "# Create a function to write the image file in local machine \n",
    "def write_file(data, filename):\n",
    "    with open(filename, 'wb', transport_params=transport_params) as f:\n",
    "        f.write(data)\n",
    "\n",
    "# Create a function to extract the feature of the image using model\n",
    "def extract_features(img_path,model):\n",
    "    #img = image.load_img(img_path,target_size=(224,224))\n",
    "    img1 = open(img_path, 'rb', transport_params=transport_params)\n",
    "    img = Image.open(img1).resize((224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "    normalized_result = result / norm(result)\n",
    "\n",
    "    return normalized_result\n",
    "\n",
    "#Create a ResNet Model\n",
    "model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
    "model.trainable = False\n",
    "\n",
    "model = tensorflow.keras.Sequential([\n",
    "    model,\n",
    "    GlobalMaxPooling2D()\n",
    "])\n",
    "\n",
    "genres=[\"Action\", \"Adventure\", \"Animation\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Family\", \"Fantasy\", \"History\", \"Horror\", \"Music\", \"Mystery\", \"Romance\", \"Science Fiction\", \"Thriller\", \"TV Movie\", \"War\", \"Western\"]\n",
    "try:\n",
    "    ctx = snowflake.connector.connect(user='SARAWANPERNETI193',password='@Temp2023',account='hb81971.ca-central-1.aws',role=\"ACCOUNTADMIN\",warehouse=\"COMPUTE_WH\",database=\"MOVIES_DB\",schema=\"MOVIES_TABLES\")\n",
    "    cursor = ctx.cursor()\n",
    "    for genre in genres:\n",
    "        feature_list =[]\n",
    "        filenumber=[]\n",
    "        sql=\"SELECT movie_id,movie_title,poster_image FROM MOVIE_DATA where movie_id > 0 and movie_id < 50000 and poster_image Is Not Null and genres like (%s)\"\n",
    "        cursor.execute(sql,(\"%\"+genre+\"%\",))\n",
    "        #cursor.execute(\"SELECT movie_id FROM movies.movie_data where movie_id < 10000 && poster_image Is Not Null\")\n",
    "        myresult = cursor.fetchall()\n",
    "        for x in tqdm(myresult):\n",
    "            filenumber.append(x[0])\n",
    "            #Provide the poster image path for the feature extraction:-\n",
    "            poster_image_path=\"azure://posters/\"+str(x[0])+\".jpg\"\n",
    "            write_file(x[2], poster_image_path)\n",
    "            feature_list.append(extract_features(poster_image_path,model))\n",
    "        print(np.array(feature_list).shape)\n",
    "        # provide the path for the feature extraction file \n",
    "        feature_extraction_file='azure://extraction/'+genre+'_imageFeaturesEmbeddings.pkl'\n",
    "        pickle.dump(feature_list,open(feature_extraction_file,'wb', transport_params=transport_params))\n",
    "        time.sleep(30)\n",
    "        print(len(filenumber))\n",
    "        # provide the path for the feature extraction file \n",
    "        feature_filenumber_file='azure://extraction/'+genre+'_imageFeaturesFileNumber.pkl'\n",
    "        pickle.dump(filenumber,open(feature_filenumber_file,'wb', transport_params=transport_params))\n",
    "        print(\"file has been loaded\")\n",
    "        time.sleep(30)\n",
    "finally:\n",
    "    cursor.close()\n",
    "ctx.close()\n",
    "print(\"program has terminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE MRS_MODEL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('LIST @ML_MODELS').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrs(session: Session) -> T.Variant:\n",
    "    #import tensorflow_hub as hub\n",
    "    import tensorflow\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "    import numpy as np\n",
    "    from numpy.linalg import norm\n",
    "    import pickle\n",
    "    import os\n",
    "    import time\n",
    "    #from tqdm import tqdm\n",
    "    from PIL import Image\n",
    "\n",
    "    from smart_open import open\n",
    "\n",
    "\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "    connect_str = 'DefaultEndpointsProtocol=https;AccountName=visrecstorage;AccountKey=q3Wvmg9bF4oPqZYdXV6PJ2+XPDfD3z4FckngdyHGMyCGE5zHMgqKPVNVk3AxGdjERc28EHGBVEE2+AStDSPpVw==;EndpointSuffix=core.windows.net'\n",
    "    transport_params = {\n",
    "        'client': BlobServiceClient.from_connection_string(connect_str),\n",
    "    }\n",
    "\n",
    "    # Create a function to write the image file in local machine \n",
    "    def write_file(data, filename):\n",
    "        with open(filename, 'wb', transport_params=transport_params) as f:\n",
    "            f.write(data)\n",
    "\n",
    "    # Create a function to extract the feature of the image using model\n",
    "    def extract_features(img_path,model):\n",
    "        #img = image.load_img(img_path,target_size=(224,224))\n",
    "        img1 = open(img_path, 'rb', transport_params=transport_params)\n",
    "        img = Image.open(img1).resize((224,224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "        preprocessed_img = preprocess_input(expanded_img_array)\n",
    "        result = model.predict(preprocessed_img).flatten()\n",
    "        normalized_result = result / norm(result)\n",
    "\n",
    "        return normalized_result\n",
    "\n",
    "    #Create a ResNet Model\n",
    "    model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
    "    model.trainable = False\n",
    "\n",
    "    model = tensorflow.keras.Sequential([\n",
    "        model,\n",
    "        GlobalMaxPooling2D()\n",
    "    ])\n",
    "\n",
    "    genres=[\"Action\", \"Adventure\", \"Animation\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Family\", \"Fantasy\", \"History\", \"Horror\", \"Music\", \"Mystery\", \"Romance\", \"Science Fiction\", \"Thriller\", \"TV Movie\", \"War\", \"Western\"]\n",
    "    try:\n",
    "        ctx = snowflake.connector.connect(user='SARAWANPERNETI193',password='@Temp2023',account='hb81971.ca-central-1.aws',role=\"ACCOUNTADMIN\",warehouse=\"COMPUTE_WH\",database=\"MOVIES_DB\",schema=\"MOVIES_TABLES\")\n",
    "        cursor = ctx.cursor()\n",
    "        for genre in genres:\n",
    "            feature_list =[]\n",
    "            filenumber=[]\n",
    "            sql=\"SELECT movie_id,movie_title,poster_image FROM MOVIE_DATA where movie_id > 0 and movie_id < 50000 and poster_image Is Not Null and genres like (%s)\"\n",
    "            cursor.execute(sql,(\"%\"+genre+\"%\",))\n",
    "            #cursor.execute(\"SELECT movie_id FROM movies.movie_data where movie_id < 10000 && poster_image Is Not Null\")\n",
    "            myresult = cursor.fetchall()\n",
    "            for x in myresult:\n",
    "                filenumber.append(x[0])\n",
    "                #Provide the poster image path for the feature extraction:-\n",
    "                poster_image_path=\"azure://posters/\"+str(x[0])+\".jpg\"\n",
    "                write_file(x[2], poster_image_path)\n",
    "                feature_list.append(extract_features(poster_image_path,model))\n",
    "            print(np.array(feature_list).shape)\n",
    "            # provide the path for the feature extraction file \n",
    "            feature_extraction_file='azure://extraction/'+genre+'_imageFeaturesEmbeddings.pkl'\n",
    "            pickle.dump(feature_list,open(feature_extraction_file,'wb', transport_params=transport_params))\n",
    "            time.sleep(30)\n",
    "            print(len(filenumber))\n",
    "            # provide the path for the feature extraction file \n",
    "            feature_filenumber_file='azure://extraction/'+genre+'_imageFeaturesFileNumber.pkl'\n",
    "            pickle.dump(filenumber,open(feature_filenumber_file,'wb', transport_params=transport_params))\n",
    "            print(\"file has been loaded\")\n",
    "            time.sleep(30)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "    ctx.close()\n",
    "    #print(\"program has terminated\")\n",
    "    result = \"program has terminated\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "package tensorflow-datasets is not installed in the local environmentYour UDF might not work when the package is installed on the server but not on your local environment.\n",
      "package keras-applications is not installed in the local environmentYour UDF might not work when the package is installed on the server but not on your local environment.\n"
     ]
    }
   ],
   "source": [
    "sproc_train_dt_model = session.sproc.register(\n",
    "                    func=mrs, \n",
    "                    name='sproc_mrs', \n",
    "                    is_permanent=True, \n",
    "                    replace=True, \n",
    "                    stage_location='@MRS_MODEL', \n",
    "                    packages=[\n",
    "                        'snowflake-snowpark-python',\n",
    "                        'scikit-learn',\n",
    "                        'joblib',\n",
    "                        'tensorflow',\n",
    "                        'keras-applications',\n",
    "                        'pillow',\n",
    "                        'smart_open',\n",
    "                        'azure-storage-blob']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"created_on\"                      |\"name\"                            |\"schema_name\"  |\"is_builtin\"  |\"is_aggregate\"  |\"is_ansi\"  |\"min_num_arguments\"  |\"max_num_arguments\"  |\"arguments\"                                         |\"description\"           |\"catalog_name\"  |\"is_table_function\"  |\"valid_for_clustering\"  |\"is_secure\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2012-08-01 00:00:00-07:00         |ASSOCIATE_SEMANTIC_CATEGORY_TAGS  |               |Y             |N               |N          |2                    |2                    |ASSOCIATE_SEMANTIC_CATEGORY_TAGS(VARCHAR, OBJEC...  |                        |                |N                    |N                       |NULL         |\n",
      "|2023-06-28 20:58:12.945000-07:00  |SPROC_MRS                         |MOVIES_TABLES  |N             |N               |N          |0                    |0                    |SPROC_MRS() RETURN VARIANT                          |user-defined procedure  |MOVIES_DB       |N                    |N                       |N            |\n",
      "|2012-08-01 00:00:00-07:00         |STORE_CLASSIFICATION              |               |Y             |N               |N          |2                    |2                    |STORE_CLASSIFICATION(VARCHAR, VARCHAR) RETURN V...  |                        |                |N                    |N                       |NULL         |\n",
      "|2012-08-01 00:00:00-07:00         |SYSTEM$SEND_EMAIL                 |               |Y             |N               |N          |4                    |4                    |SYSTEM$SEND_EMAIL(VARCHAR, VARCHAR, VARCHAR, VA...  |                        |                |N                    |N                       |NULL         |\n",
      "|2012-08-01 00:00:00-07:00         |SYSTEM$SEND_EMAIL                 |               |Y             |N               |N          |5                    |5                    |SYSTEM$SEND_EMAIL(VARCHAR, VARCHAR, VARCHAR, VA...  |                        |                |N                    |N                       |NULL         |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('show procedures').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to execute query [queryID: 01ad4898-3200-cc62-0003-5d520002a10e] CALL sproc_mrs()\n",
      "100357 (P0000): Python Interpreter Error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1354, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1256, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1302, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1251, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1011, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 951, in send\n",
      "    self.connect()\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1418, in connect\n",
      "    super().connect()\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 922, in connect\n",
      "    self.sock = self._create_connection(\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/socket.py\", line 787, in create_connection\n",
      "    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno -3] Temporary failure in name resolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 300, in get_file\n",
      "    urlretrieve(origin, fpath, DLProgbar())\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 84, in urlretrieve\n",
      "    response = urlopen(url, data)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 542, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1397, in https_open\n",
      "    return self.do_open(http.client.HTTPSConnection, req,\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1357, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"_udf_code.py\", line 7, in compute\n",
      "  File \"C:\\Users\\ds.perneti\\AppData\\Local\\Temp\\ipykernel_15008\\1508263901.py\", line 44, in mrs\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/applications/resnet.py\", line 521, in ResNet50\n",
      "    return ResNet(\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/applications/resnet.py\", line 232, in ResNet\n",
      "    weights_path = data_utils.get_file(\n",
      "  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 304, in get_file\n",
      "    raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
      "Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5: None -- [Errno -3] Temporary failure in name resolution\n",
      " in function SPROC_MRS with handler compute\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1354, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1418, in connect\n    super().connect()\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 922, in connect\n    self.sock = self._create_connection(\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/socket.py\", line 787, in create_connection\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 300, in get_file\n    urlretrieve(origin, fpath, DLProgbar())\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 84, in urlretrieve\n    response = urlopen(url, data)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 222, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 525, in open\n    response = self._open(req, data)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 542, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n    result = func(*args)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1397, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1357, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"_udf_code.py\", line 7, in compute\n  File \"C:\\Users\\ds.perneti\\AppData\\Local\\Temp\\ipykernel_15008\\1508263901.py\", line 44, in mrs\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/applications/resnet.py\", line 521, in ResNet50\n    return ResNet(\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/applications/resnet.py\", line 232, in ResNet\n    weights_path = data_utils.get_file(\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 304, in get_file\n    raise Exception(error_msg.format(origin, e.errno, e.reason))\nException: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5: None -- [Errno -3] Temporary failure in name resolution\n in function SPROC_MRS with handler compute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[43msproc_train_dt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\stored_procedure.py:72\u001b[0m, in \u001b[0;36mStoredProcedure.__call__\u001b[1;34m(self, session, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_types) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect number of arguments passed to the stored procedure. Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_types)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\session.py:1591\u001b[0m, in \u001b[0;36mSession.call\u001b[1;34m(self, sproc_name, *args)\u001b[0m\n\u001b[0;32m   1589\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msproc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sql_args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1590\u001b[0m set_api_call_source(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession.call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\telemetry.py:132\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mquery_history() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[1;32m--> 132\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     api_calls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mapi_calls,\n\u001b[0;32m    135\u001b[0m         {TelemetryField\u001b[38;5;241m.\u001b[39mNAME\u001b[38;5;241m.\u001b[39mvalue: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    136\u001b[0m     ]\n\u001b[0;32m    137\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39m_telemetry_client\u001b[38;5;241m.\u001b[39msend_function_usage_telemetry(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    139\u001b[0m         TelemetryField\u001b[38;5;241m.\u001b[39mFUNC_CAT_ACTION\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m    140\u001b[0m         api_calls\u001b[38;5;241m=\u001b[39mapi_calls,\n\u001b[0;32m    141\u001b[0m         sfqids\u001b[38;5;241m=\u001b[39m[q\u001b[38;5;241m.\u001b[39mquery_id \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query_history\u001b[38;5;241m.\u001b[39mqueries],\n\u001b[0;32m    142\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:486\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self, statement_params, block)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[0;32m    481\u001b[0m     warning(\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect.block\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock argument is experimental. Do not use it in production.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    484\u001b[0m     )\n\u001b[1;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_collect_with_tag_no_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:520\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[1;34m(self, statement_params, block, data_type)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:417\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_in_stored_procedure() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsync query is not supported in stored procedure yet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m     )\n\u001b[1;32m--> 417\u001b[0m result_set, result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_set\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:148\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[0;32m    146\u001b[0m         e\n\u001b[0;32m    147\u001b[0m     )\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:81\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:505\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m holder, id_ \u001b[38;5;129;01min\u001b[39;00m placeholders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    504\u001b[0m     final_query \u001b[38;5;241m=\u001b[39m final_query\u001b[38;5;241m.\u001b[39mreplace(holder, id_)\n\u001b[1;32m--> 505\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m placeholders[query\u001b[38;5;241m.\u001b[39mquery_id_place_holder] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    515\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mquery_id\n\u001b[0;32m    516\u001b[0m )\n\u001b[0;32m    517\u001b[0m result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor\u001b[38;5;241m.\u001b[39mdescription\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:105\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    102\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:99\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    102\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[0;32m    103\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:345\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     query_id_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    344\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:329\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_statement_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m--> 329\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[0;32m    331\u001b[0m         QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery)\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    333\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\connector\\cursor.py:904\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[1;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[0;32m    900\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    901\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    902\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[0;32m    903\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[1;32m--> 904\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\connector\\errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[0;32m    269\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[0;32m    298\u001b[0m             error_class,\n\u001b[0;32m    299\u001b[0m             error_value,\n\u001b[0;32m    300\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\connector\\errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 345\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pysnowpark\\lib\\site-packages\\snowflake\\connector\\errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    219\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[0;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[0;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[0;32m    229\u001b[0m     ),\n\u001b[0;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[0;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[0;32m    232\u001b[0m )\n",
      "\u001b[1;31mSnowparkSQLException\u001b[0m: (1304): 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1354, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 1418, in connect\n    super().connect()\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/http/client.py\", line 922, in connect\n    self.sock = self._create_connection(\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/socket.py\", line 787, in create_connection\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 300, in get_file\n    urlretrieve(origin, fpath, DLProgbar())\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 84, in urlretrieve\n    response = urlopen(url, data)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 222, in urlopen\n    return opener.open(url, data, timeout)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 525, in open\n    response = self._open(req, data)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 542, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n    result = func(*args)\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1397, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/urllib/request.py\", line 1357, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"_udf_code.py\", line 7, in compute\n  File \"C:\\Users\\ds.perneti\\AppData\\Local\\Temp\\ipykernel_15008\\1508263901.py\", line 44, in mrs\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/applications/resnet.py\", line 521, in ResNet50\n    return ResNet(\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/applications/resnet.py\", line 232, in ResNet\n    weights_path = data_utils.get_file(\n  File \"/usr/lib/python_udf/8ecf914fb80eb1bda2e5044e222a41ad44603ddefe8937cd35de3d2eb0c20334/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 304, in get_file\n    raise Exception(error_msg.format(origin, e.errno, e.reason))\nException: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5: None -- [Errno -3] Temporary failure in name resolution\n in function SPROC_MRS with handler compute"
     ]
    }
   ],
   "source": [
    "model_response = sproc_train_dt_model(session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHSMQ1LwYm6P"
   },
   "source": [
    "Note:- The above code is a once time execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5HC7gjUas4W"
   },
   "source": [
    "##4.Similarity calculation:\n",
    "The 'brute' algorithm has been used to determine the nearest neighbor images for the reference, using the Euclidean distance metric to measure visual similarity. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DXtkBnDbxB-"
   },
   "source": [
    "###`Euclidean distance`\n",
    "\n",
    "also known as Euclidean metric, is a measure of the straight-line distance between two points in Euclidean space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI_cGo6Ab9qU"
   },
   "source": [
    "###`Brute-Force Algorithm`\n",
    "\n",
    "also known as an exhaustive search algorithm, is a straightforward approach to problem-solving that systematically tries every possible solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VzxUxMwmttQ"
   },
   "source": [
    "## 5.Ranking & recommendation:\n",
    "The best 5 visually similar images from each following genre of the reference film are suggested using the scores from the Euclidean distance metric computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUjYwkzwu78G"
   },
   "source": [
    "# Movie Recommendation Front end application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi2YUpEVvgMl"
   },
   "source": [
    "The front end application has been developed using the Streamlit platform\n",
    "\n",
    "To view the Streamlit app on a browser, run it with the following\n",
    "  command:\n",
    "\n",
    "    streamlit run pythonfile\n",
    "\n",
    "Example:-\n",
    "\n",
    "    streamlit run c:\\users\\vivek.kakumanu\\desktop\\python_learnings\\python_script\\project\\poster_recomendation_system\\movie_recomendation_based_on_genres.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFS_ptYQcMxh"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.linalg import norm\n",
    "import snowflake.connector\n",
    "import random\n",
    "\n",
    "from smart_open import open\n",
    "\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=visrecstorage;AccountKey=q3Wvmg9bF4oPqZYdXV6PJ2+XPDfD3z4FckngdyHGMyCGE5zHMgqKPVNVk3AxGdjERc28EHGBVEE2+AStDSPpVw==;EndpointSuffix=core.windows.net'\n",
    "transport_params = {\n",
    "    'client': BlobServiceClient.from_connection_string(connect_str),\n",
    "}\n",
    "\n",
    "st.title('Movie Poster Recommender System')\n",
    "\n",
    "\n",
    "#Create a file method \n",
    "def file_name(uploaded_file):\n",
    "    return \"azure://uploads/\"+ str(uploaded_file)+\".jpg\"\n",
    "\n",
    "\n",
    "#Create a save file method \n",
    "def save_uploaded_file(data, uploaded_file):\n",
    "    try:\n",
    "        with open(file_name(uploaded_file),'wb', transport_params=transport_params) as f:\n",
    "            f.write(data)\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Create a function to extract the feature of the image using model\n",
    "def feature_extraction(img_path,model):\n",
    "    img1 = open(img_path, 'rb', transport_params=transport_params)\n",
    "    img = Image.open(img1).resize((224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "    normalized_result = result / norm(result)\n",
    "\n",
    "    return normalized_result\n",
    "\n",
    "# Create a function to recommend the images based on the features extract by the model.\n",
    "def recommend(features,genre):\n",
    "  # provide the path for the feature extraction file \n",
    "    feature_extraction_file='azure://extraction/'+genre+'_imageFeaturesEmbeddings.pkl'        \n",
    "    feature_list = np.array(pickle.load(open(feature_extraction_file,'rb', transport_params=transport_params)))\n",
    "    neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')\n",
    "    neighbors.fit(feature_list)\n",
    "\n",
    "    distances, indices = neighbors.kneighbors([features])\n",
    "\n",
    "    return indices\n",
    "\n",
    "model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
    "model.trainable = False\n",
    "\n",
    "model = tensorflow.keras.Sequential([\n",
    "    model,\n",
    "    GlobalMaxPooling2D()\n",
    "])\n",
    "\n",
    "\n",
    "def predict_movies(movie_id):\n",
    "    # feature extract\n",
    "        features = feature_extraction(file_name(movie_id),model)\n",
    "        sql = \"SELECT movie_title, genres FROM MOVIE_DATA where movie_id = \"+str(movie_id)\n",
    "        print(sql)\n",
    "        cursor.execute(sql)\n",
    "        txt=cursor.fetchall()\n",
    "        print(txt[0][1])\n",
    "        genres = txt[0][1].split('-')\n",
    "        st.write(txt[0][0])\n",
    "        st.write(txt[0][1])\n",
    "        for genre in genres:\n",
    "            # recommendention\n",
    "            indices = recommend(features,genre)\n",
    "            where_in = ','.join(['%s'] * len(indices[0]))\n",
    "            # provide the path for the feature extraction file \n",
    "            feature_filenumber_file='azure://extraction/'+genre+'_imageFeaturesFileNumber.pkl'\n",
    "            filenames1 = pickle.load(open(feature_filenumber_file,'rb', transport_params=transport_params))\n",
    "            sql = \"SELECT movie_id,movie_title,poster_image FROM movie_data where movie_id in (%s)\" % (where_in)\n",
    "            sql = sql+ \"and genres like (%s) and movie_id not in (%s)\"\n",
    "            indices_list=[]\n",
    "            length = len(indices[0])\n",
    "            for i in range(0,length):\n",
    "                indices_list.append(filenames1[indices[0][i]])\n",
    "            tuple_list = tuple(indices_list) + (\"%\"+genre+\"%\", movie_id,)\n",
    "            cursor.execute(sql,tuple_list)\n",
    "            recomended_results = cursor.fetchall()\n",
    "            recomended_result=[]\n",
    "            for i in range (0,len(recomended_results)):\n",
    "                for j in range(length):\n",
    "                    if indices_list[j]==recomended_results[i][0]:\n",
    "                        recomended_result.append(recomended_results[i])\n",
    "                if len(recomended_result)==6:\n",
    "                    break\n",
    "            st.header(genre)\n",
    "            col1,col2,col3,col4,col5 = st.columns(5)\n",
    "            \n",
    "            with col1:\n",
    "                if save_uploaded_file(recomended_result[0][2],recomended_result[0][0]):\n",
    "                # display the file\n",
    "                    display_image0 = open(file_name(recomended_result[0][0]), 'rb', transport_params=transport_params)\n",
    "                    display_image = Image.open(display_image0)\n",
    "                    st.image(display_image)\n",
    "                    st.write(recomended_result[0][1])\n",
    "            with col2:\n",
    "                if save_uploaded_file(recomended_result[1][2],recomended_result[1][0]):\n",
    "                # display the file\n",
    "                    display_image1 = open(file_name(recomended_result[1][0]), 'rb', transport_params=transport_params)\n",
    "                    display_image = Image.open(display_image1)\n",
    "                    st.image(display_image)\n",
    "                    st.write(recomended_result[1][1])\n",
    "            with col3:\n",
    "                if save_uploaded_file(recomended_result[2][2],recomended_result[2][0]):\n",
    "                # display the file\n",
    "                    display_image2 = open(file_name(recomended_result[2][0]), 'rb', transport_params=transport_params)\n",
    "                    display_image = Image.open(display_image2)\n",
    "                    st.image(display_image)\n",
    "                    st.write(recomended_result[2][1])\n",
    "            with col4:\n",
    "                if save_uploaded_file(recomended_result[3][2],recomended_result[3][0]):\n",
    "                # display the file\n",
    "                    display_image3 = open(file_name(recomended_result[3][0]), 'rb', transport_params=transport_params)\n",
    "                    display_image = Image.open(display_image3)\n",
    "                    st.image(display_image)\n",
    "                    st.write(recomended_result[3][1]) \n",
    "            with col5:\n",
    "                if save_uploaded_file(recomended_result[4][2],recomended_result[4][0]):\n",
    "                # display the file\n",
    "                    display_image4 = open(file_name(recomended_result[4][0]), 'rb', transport_params=transport_params)\n",
    "                    display_image = Image.open(display_image4)\n",
    "                    st.image(display_image)\n",
    "                    st.write(recomended_result[4][1])\n",
    "\n",
    "if \"refreshclick\" not in st.session_state:\n",
    "    st.session_state.refreshclick=False\n",
    "    if \"movie_id\" not in st.session_state:\n",
    "        randomlist=[]\n",
    "        for i in range(0,30):\n",
    "            n = random.randint(1,122)\n",
    "            randomlist.append(n)\n",
    "        st.session_state.movie_id=randomlist\n",
    "\n",
    "        \n",
    "try:\n",
    "    ctx = snowflake.connector.connect(user='SARAWANPERNETI193',password='@Temp2023',account='hb81971.ca-central-1.aws',role=\"ACCOUNTADMIN\",warehouse=\"COMPUTE_WH\",database=\"MOVIES_DB\",schema=\"MOVIES_TABLES\")\n",
    "    cursor = ctx.cursor()\n",
    "    print (\"randon number\",tuple(st.session_state.movie_id))\n",
    "    where_in = ','.join(['%s'] * len(st.session_state['movie_id']))\n",
    "    sql = \"SELECT movie_id,movie_title,poster_image FROM movie_data where movie_id in (%s) \" % (where_in)\n",
    "    sql = sql+ \"and genres not like (%s) and genres not like (%s) and genres not like (%s)\"\n",
    "    print(sql)\n",
    "    tuple_list = tuple(st.session_state.movie_id) + (\"%TV Movie%\",\"%Romance%\",\"%Drama%\")\n",
    "    print(len(tuple_list))\n",
    "    cursor.execute(sql, tuple_list)\n",
    "    myresult = cursor.fetchall()\n",
    "    col1,col2,col3,col4,col5 = st.columns(5)\n",
    "    st.session_state.refreshclick = False\n",
    "    if(len(myresult)!=0):\n",
    "        with col1:\n",
    "            if save_uploaded_file(myresult[0][2],myresult[0][0]):\n",
    "            # display the file\n",
    "                display_image0 = open(file_name(myresult[0][0]), 'rb', transport_params=transport_params)\n",
    "                display_image = Image.open(display_image0)\n",
    "                clicked_0 = st.button(myresult[0][1] , st.image(display_image))\n",
    "        with col2:\n",
    "            if save_uploaded_file(myresult[1][2],myresult[1][0]):\n",
    "            # display the file\n",
    "                display_image1 = open(file_name(myresult[1][0]), 'rb', transport_params=transport_params)\n",
    "                display_image = Image.open(display_image1)\n",
    "                clicked_1 = st.button(myresult[1][1] , st.image(display_image))\n",
    "\n",
    "        with col3:\n",
    "            if save_uploaded_file(myresult[2][2],myresult[2][0]):\n",
    "            # display the file\n",
    "                display_image2 = open(file_name(myresult[2][0]), 'rb', transport_params=transport_params)\n",
    "                display_image = Image.open(display_image2)\n",
    "                clicked_2 = st.button(myresult[2][1] , st.image(display_image))\n",
    "        with col4:\n",
    "            if save_uploaded_file(myresult[3][2],myresult[3][0]):\n",
    "            # display the file\n",
    "                display_image3 = open(file_name(myresult[3][0]), 'rb', transport_params=transport_params)\n",
    "                display_image = Image.open(display_image3)\n",
    "                clicked_3 = st.button(myresult[3][1] , st.image(display_image))\n",
    "\n",
    "        with col5:\n",
    "            if save_uploaded_file(myresult[4][2],myresult[4][0]):\n",
    "            # display the file\n",
    "                display_image4 = open(file_name(myresult[4][0]), 'rb', transport_params=transport_params)\n",
    "                display_image = Image.open(display_image4)\n",
    "                clicked_4 = st.button(myresult[4][1] , st.image(display_image))\n",
    "\n",
    "        if(clicked_0):\n",
    "            predict_movies(myresult[0][0])\n",
    "        if(clicked_1):\n",
    "            predict_movies(myresult[1][0])\n",
    "        if(clicked_2):\n",
    "            predict_movies(myresult[2][0])\n",
    "        if(clicked_3):\n",
    "            predict_movies(myresult[3][0])\n",
    "        if(clicked_4):\n",
    "            predict_movies(myresult[4][0])\n",
    "finally:\n",
    "    cursor.close()\n",
    "ctx.close()\n",
    "\n",
    "clicked = st.button(\"Refresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !streamlit run mrs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run mrssf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install azure-storage-blob azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
